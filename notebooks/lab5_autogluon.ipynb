{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ecxO9uZwcTAa"
   },
   "source": [
    "<img width=\"30%\" src=\"https://user-images.githubusercontent.com/16392542/77208906-224aa500-6aba-11ea-96bd-e81806074030.png\" alt=\"AutoGluon logo\" style=\"display: block; margin-left: auto; margin-right: auto;\">\n",
    "\n",
    "# Lab 5: time series forecast with AutoGluon\n",
    "\n",
    "[AutoGluon](https://auto.gluon.ai/stable/index.html) is an open-source AutoML library that automates machine learning and deep learning for applications involving image, text, and tabular data. It enables users to quickly develop high-performance models with just a few lines of code by automating tasks like data preprocessing, model selection, hyperparameter tuning, and ensemble learning.\n",
    "\n",
    "[AutoGluon Time Series](https://auto.gluon.ai/stable/tutorials/timeseries/index.html) can forecast the future values of multiple time series given the historical data and other related covariates. A single call to AutoGluon [`TimeSeriesPredictor`](https://auto.gluon.ai/stable/api/autogluon.timeseries.TimeSeriesPredictor.html)’s `fit()` method trains multiple models to generate accurate probabilistic forecasts, and does not require you to manually deal with cumbersome issues like model selection and hyperparameter tuning.\n",
    "\n",
    "Under the hood, AutoGluon combines various state of the art forecasting algorithms. These include established statical methods like ETS and ARIMA from [`StatsForecast`](https://github.com/Nixtla/statsforecast), efficient tree-based forecasters like LightGBM based on [AutoGluon-Tabular](https://auto.gluon.ai/stable/tutorials/tabular/index.html), flexible deep learning models like [DeepAR](https://docs.aws.amazon.com/sagemaker/latest/dg/deepar.html) and Temporal Fusion Transformer from [GluonTS](https://ts.gluon.ai/), and a pretrained zero-shot forecasting model, [Chronos](https://github.com/amazon-science/chronos-forecasting). \n",
    "\n",
    "This workshop features DeepAR algorithm in the notebook [lab 3](./lab3_sagemaker_deepar.ipynb) and Chronos in the notebook [lab 4](./lab4_chronos.ipynb).\n",
    "\n",
    "AutoGluon Time Series provides a robust and easy way to use Chronos through the `TimeSeriesPredictor` API. More specifically:\n",
    "- Chronos can be combined with other forecasting models to build accurate ensembles using the `high_quality` and `best_quality` presets.\n",
    "- Alternatively, Chronos can be used as a standalone zero-shot model with presets such as `\"chronos_small\"` or `\"chronos_base\"`.\n",
    "\n",
    "For the full list of available time series algorithm used in AutoGluon refer to the [Model Zoo](https://auto.gluon.ai/stable/tutorials/timeseries/forecasting-model-zoo.html) page in the AutoGluon documentation.\n",
    "\n",
    "This notebook demonstrate how to build time series forecast using AutoGluon Time Series API.\n",
    "\n",
    "If you're not familiar with time series forecast, refer to [Forecasting Time Series in Depth](https://auto.gluon.ai/stable/tutorials/timeseries/forecasting-indepth.html) section in the AutoGluon documentation for more details and background."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"data:image/svg+xml;base64,Cjxzdmcgd2lkdGg9IjgwMCIgaGVpZ2h0PSIxMjUiIHZpZXdCb3g9IjAgMCA4MDAgMTI1IiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciPgogICAgPGRlZnM+CiAgICAgICAgPGxpbmVhckdyYWRpZW50IGlkPSJmYWRlR3JhZGllbnQiIHgxPSIwIiB4Mj0iMSI+CiAgICAgICAgICAgIDxzdG9wIG9mZnNldD0iMCUiIHN0b3AtY29sb3I9IiNGMEYwRjAiLz4KICAgICAgICAgICAgPHN0b3Agb2Zmc2V0PSIxMDAlIiBzdG9wLWNvbG9yPSIjRjBGMEYwIiBzdG9wLW9wYWNpdHk9IjAiLz4KICAgICAgICA8L2xpbmVhckdyYWRpZW50PgogICAgICAgIDxtYXNrIGlkPSJmYWRlTWFzayI+CiAgICAgICAgICAgIDxyZWN0IHg9IjAiIHk9IjAiIHdpZHRoPSI3NTAiIGhlaWdodD0iMTI1IiBmaWxsPSJ3aGl0ZSIvPgogICAgICAgICAgICA8cmVjdCB4PSI3NTAiIHk9IjAiIHdpZHRoPSI1MCIgaGVpZ2h0PSIxMjUiIGZpbGw9InVybCgjZmFkZUdyYWRpZW50KSIvPgogICAgICAgIDwvbWFzaz4KICAgIDwvZGVmcz4KICAgIDxwYXRoIGQ9Ik0zLDUwIEE1MCw1MCAwIDAgMSA1MywzIEw3OTcsMyBMNzk3LDk3IEw5Nyw5NyBMNTAsMTE1IEwzLDk3IFoiIGZpbGw9IiNGMEYwRjAiIHN0cm9rZT0iI0UwRTBFMCIgc3Ryb2tlLXdpZHRoPSIxIiBtYXNrPSJ1cmwoI2ZhZGVNYXNrKSIvPgogICAgPGNpcmNsZSBjeD0iNTAiIGN5PSI1MCIgcj0iMzAiIGZpbGw9IiM1N2M0ZjgiIHN0cm9rZT0iIzU3YzRmOCIgc3Ryb2tlLXdpZHRoPSIxIi8+CiAgICA8Y2lyY2xlIGN4PSI1MCIgY3k9IjUwIiByPSIyNSIgZmlsbD0iI0YwRjBGMCIvPgogICAgPGxpbmUgeDE9IjUwIiB5MT0iNTAiIHgyPSI1MCIgeTI9IjMwIiBzdHJva2U9IiM1N2M0ZjgiIHN0cm9rZS13aWR0aD0iMyIgc3Ryb2tlLWxpbmVjYXA9InJvdW5kIi8+CiAgICA8bGluZSB4MT0iNTAiIHkxPSI1MCIgeDI9IjY1IiB5Mj0iNTAiIHN0cm9rZT0iIzU3YzRmOCIgc3Ryb2tlLXdpZHRoPSIzIiBzdHJva2UtbGluZWNhcD0icm91bmQiLz4KICAgIDx0ZXh0IHg9IjEwMCIgeT0iMzQiIGZvbnQtZmFtaWx5PSJBcmlhbCwgc2Fucy1zZXJpZiIgZm9udC1zaXplPSIxNCIgZmlsbD0iIzMzMzMzMyI+VGhlIG5leHQgY2VsbCBtYXkgdGFrZSBhIGZldyBtaW51dGVzIHRvIHJ1bi4gUGxlYXNlIGJlIHBhdGllbnQuPC90ZXh0PgogICAgPHRleHQgeD0iMTAwIiB5PSI1NiIgZm9udC1mYW1pbHk9IkFyaWFsLCBzYW5zLXNlcmlmIiBmb250LXNpemU9IjE0IiBmaWxsPSIjMzMzMzMzIj5JZ25vcmUgdGhlIHdhcm5pbmdzIGFuZCBlcnJvcnMsIGFsb25nIHdpdGggdGhlIG5vdGUgYWJvdXQgcmVzdGFydGluZyB0aGUga2VybmVsIGF0IHRoZSBlbmQuPC90ZXh0Pgo8L3N2Zz4K\" alt=\"Time alert open medium\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf /opt/conda/lib/python3.10/site-packages/fsspec*\n",
    "%pip install -q fsspec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mdbOpdQucTAf",
    "tags": [
     "remove-cell",
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "%pip install -Uq autogluon pip sagemaker s3fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# downgrade sentencepiece to 0.1.99 if you on SageMaker Distribution Image (SMD) 2.0\n",
    "# this is fixed in SMD > 2.1\n",
    "# %pip install sentencepiece==0.1.99"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"data:image/svg+xml;base64,Cjxzdmcgd2lkdGg9IjgwMCIgaGVpZ2h0PSI1MCIgdmlld0JveD0iMCAwIDgwMCA1MCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICAgIDxkZWZzPgogICAgICAgIDxsaW5lYXJHcmFkaWVudCBpZD0iZmFkZUdyYWRpZW50IiB4MT0iMCIgeDI9IjEiPgogICAgICAgICAgICA8c3RvcCBvZmZzZXQ9IjAlIiBzdG9wLWNvbG9yPSIjRjBGMEYwIi8+CiAgICAgICAgICAgIDxzdG9wIG9mZnNldD0iMTAwJSIgc3RvcC1jb2xvcj0iI0YwRjBGMCIgc3RvcC1vcGFjaXR5PSIwIi8+CiAgICAgICAgPC9saW5lYXJHcmFkaWVudD4KICAgICAgICA8bWFzayBpZD0iZmFkZU1hc2siPgogICAgICAgICAgICA8cmVjdCB4PSIwIiB5PSIwIiB3aWR0aD0iNzUwIiBoZWlnaHQ9IjUwIiBmaWxsPSJ3aGl0ZSIvPgogICAgICAgICAgICA8cmVjdCB4PSI3NTAiIHk9IjAiIHdpZHRoPSI1MCIgaGVpZ2h0PSI1MCIgZmlsbD0idXJsKCNmYWRlR3JhZGllbnQpIi8+CiAgICAgICAgPC9tYXNrPgogICAgPC9kZWZzPgogICAgPHBhdGggZD0iTTI1LDUwIFEwLDUwIDAsMjUgTDUwLDMgTDk3LDI1IEw3OTcsMjUgTDc5Nyw1MCBMMjUsNTAgWiIgZmlsbD0iI0YwRjBGMCIgc3Ryb2tlPSIjRTBFMEUwIiBzdHJva2Utd2lkdGg9IjEiIG1hc2s9InVybCgjZmFkZU1hc2spIi8+Cjwvc3ZnPgo=\" alt=\"Time alert close\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment if you need to restart kernel to get the packages\n",
    "# import IPython\n",
    "# IPython.Application.instance().kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uhkp-GwlcTAh"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import json\n",
    "import os\n",
    "import boto3\n",
    "import zipfile\n",
    "from time import gmtime, strftime, sleep\n",
    "import datetime\n",
    "import sagemaker\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tqdm\n",
    "import torch\n",
    "import autogluon\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import (\n",
    "interact, interactive, fixed, interact_manual,\n",
    "IntSlider, FloatSlider, Checkbox, Dropdown, DatePicker, SelectMultiple, Select\n",
    ")\n",
    "\n",
    "from autogluon.timeseries import TimeSeriesDataFrame, TimeSeriesPredictor\n",
    "from autogluon.timeseries.splitter import ExpandingWindowSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib.metadata import version\n",
    "print(version('autogluon'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set literals and general variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get domain_id and user profile name\n",
    "NOTEBOOK_METADATA_FILE = \"/opt/ml/metadata/resource-metadata.json\"\n",
    "domain_id = None\n",
    "\n",
    "if os.path.exists(NOTEBOOK_METADATA_FILE):\n",
    "    with open(NOTEBOOK_METADATA_FILE, \"rb\") as f:\n",
    "        domain_id = json.loads(f.read()).get('DomainId')\n",
    "        print(f\"SageMaker domain id: {domain_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_session = sagemaker.Session()\n",
    "region = sagemaker_session.boto_region_name\n",
    "sm = boto3.client(\"sagemaker\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_bucket = sagemaker_session.default_bucket()  # replace with an existing bucket if needed\n",
    "s3_prefix = \"autogluon-demo-notebook\"  # prefix used for all data stored within the bucket\n",
    "experiment_prefix = \"autogluon\"\n",
    "extract_to_path = './data'\n",
    "\n",
    "sm_role = sagemaker.get_execution_role()  # IAM role to use by SageMaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_data_path = f\"s3://{s3_bucket}/{s3_prefix}/data\"\n",
    "s3_output_path = f\"s3://{s3_bucket}/{s3_prefix}/output\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the dataset\n",
    "\n",
    "Download the from the SageMaker example S3 bucket. You use the [electricity dataset](https://archive.ics.uci.edu/ml/datasets/ElectricityLoadDiagrams20112014) from the repository of the University of California, Irvine:\n",
    "> Trindade, Artur. (2015). ElectricityLoadDiagrams20112014. UCI Machine Learning Repository. https://doi.org/10.24432/C58C86."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(extract_to_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_zip_file_name = 'LD2011_2014.txt.zip'\n",
    "dataset_path = f'{extract_to_path}/LD2011_2014.txt'\n",
    "\n",
    "s3_dataset_path = f\"datasets/timeseries/uci_electricity/{dataset_zip_file_name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isfile(dataset_path):\n",
    "    print(f'Downloading and unzipping the dataset to {dataset_path}')\n",
    "    s3_client = boto3.client(\"s3\")\n",
    "    s3_client.download_file(\n",
    "        f\"sagemaker-example-files-prod-{region}\", s3_dataset_path, f\"{extract_to_path}/{dataset_zip_file_name}\"\n",
    "    )\n",
    "\n",
    "    with zipfile.ZipFile(f\"{extract_to_path}/{dataset_zip_file_name}\", \"r\") as zip_ref:\n",
    "        total_size = sum(file.file_size for file in zip_ref.infolist())\n",
    "\n",
    "        with tqdm.tqdm(total=total_size, unit='B', unit_scale=True, desc=\"Extracting\") as pbar:\n",
    "            for file in zip_ref.infolist():\n",
    "                zip_ref.extract(file, extract_to_path)\n",
    "                pbar.update(file.file_size)\n",
    "        \n",
    "    dataset_path = '.'.join(zip_ref.filename.split('.')[:-1])\n",
    "else:\n",
    "    print(f'The dataset {dataset_path} exists, skipping download and unzip!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see what is inside the file\n",
    "# !head -n 2 {dataset_path} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore and preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw = pd.read_csv(\n",
    "    dataset_path, \n",
    "    sep=';', \n",
    "    index_col=0,\n",
    "    decimal=',',\n",
    "    parse_dates=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resample to 2h intervals\n",
    "freq = \"2h\"\n",
    "div = 8 # 2 hours contain 8x 15 min intervals, you need to  delete the resampled value by 8\n",
    "num_timeseries = df_raw.shape[1]\n",
    "data_kw = df_raw.resample(freq).sum() / div\n",
    "timeseries = []\n",
    "\n",
    "for i in tqdm.trange(num_timeseries):\n",
    "    timeseries.append(np.trim_zeros(data_kw.iloc[:, i], trim=\"f\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_kw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show random timeseries for 28 day period\n",
    "sample_size = 10\n",
    "fig, axs = plt.subplots(5, 2, figsize=(20, 20), sharex=True)\n",
    "axx = axs.ravel()\n",
    "for i, ts in tqdm.tqdm(enumerate(random.sample(timeseries, sample_size)), total=sample_size, desc=\"Creating plots\"):\n",
    "    series = ts.loc[\"2014-01-01\":\"2014-01-28\"]\n",
    "    if len(series): series.plot(ax=axx[i])\n",
    "    axx[i].set_xlabel(\"date\")\n",
    "    axx[i].set_ylabel(f\"kW consumption - {ts.name}\")\n",
    "    axx[i].grid(which=\"minor\", axis=\"x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a smaller dataset with a subset of time series:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select two random time series to include in a small dataset\n",
    "sample_size = 2\n",
    "columns_to_keep = np.random.choice(data_kw.columns.to_list(), size=sample_size, replace=False)\n",
    "columns_to_keep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_kw_small = data_kw[columns_to_keep]\n",
    "data_kw_small"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train AutoGluon models\n",
    "\n",
    "To start with AutoGluon you need:\n",
    "1. Convert your time series dataset to a [`TimeSeriesDataFrame`](https://auto.gluon.ai/stable/api/autogluon.timeseries.TimeSeriesDataFrame.html)\n",
    "2. Configure AutoGluon [`TimeSeriesPredictor`](https://auto.gluon.ai/stable/api/autogluon.timeseries.TimeSeriesPredictor.html)\n",
    "3. Fit models\n",
    "\n",
    "This section takes you through these steps so that you have a trained predictor.\n",
    "\n",
    "Refere to the [AutoGluon Time Series - Forecasting Quick Start](https://auto.gluon.ai/stable/tutorials/timeseries/forecasting-quick-start.html) documentation for details on time series forecasting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict for 7 days\n",
    "prediction_days = 7\n",
    "intervals_per_day = 12\n",
    "prediction_length = prediction_days * intervals_per_day\n",
    "\n",
    "print(f\"Sampling frequency set to {freq}. Generate predictions for {prediction_length} intervals\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert dataset to AutoGluon format\n",
    "\n",
    "AutoGluon expects time series data in [long format](https://doc.dataiku.com/dss/latest/time-series/data-formatting.html#long-format). Each row of the data frame contains a single observation (timestep) of a single time series represented by:\n",
    "- unique ID of the time series – `item_id` – as int or str  \n",
    "- timestamp of the observation – `timestamp` – as a `pandas.Timestamp` or compatible format\n",
    "- numeric value of the time series – `target`\n",
    "\n",
    "You can choose the names of the columns arbitrary, but need to specify the names when constructing a `TimeSeriesDataFrame` object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "To reduce the training time and inference time you can use a subset of time series instead of the full dataset with 370 time series.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For AWS-led workshop please choose a small dataset by leaving the `USE_FULL_DATASET` on it's default value `False`. The training time for the small dataset is about 5-12 minutes depending on the used JupyterLab App instance. The train time for the full dataset is about 40-70 minutes depending on the instance type. See the section **Train models** for more data points on training times on different SageMaker instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_FULL_DATASET = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.device_count() < 1 and USE_FULL_DATASET:\n",
    "    print(f\"\\033[91mWARNING: You don't use GPU instance for the notebook. The inference time on CPU instance will be not practical.\\033[0m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if USE_FULL_DATASET:\n",
    "    ts_wide_df = data_kw\n",
    "else:\n",
    "    ts_wide_df = data_kw_small"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the [`TimeSeriesDataFrame`](https://auto.gluon.ai/stable/api/autogluon.timeseries.TimeSeriesDataFrame.html) classs to convert a Pandas DataFrame to AutoGluon time series Data Frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converts a Pandas.DataFrame to AutoGluon TimeSeriesDataFrame\n",
    "# note the function hardcodes column names and work only with the dataset in this notebook\n",
    "def get_ts_df(\n",
    "    df,\n",
    ") ->  TimeSeriesDataFrame: \n",
    "    # Convert to an AutoGluon TimeSeriesDataFrame\n",
    "    return TimeSeriesDataFrame.from_data_frame(\n",
    "        # Melt the DataFrame into the long format\n",
    "        pd.melt(\n",
    "            df.reset_index(),\n",
    "            id_vars='index', \n",
    "            value_vars=df.columns, \n",
    "            var_name='item_id', \n",
    "            value_name='target'\n",
    "        ).rename(columns={'index': 'timestamp'}), # Rename the 'index' column to 'timestamp'\n",
    "        id_column=\"item_id\",\n",
    "        timestamp_column=\"timestamp\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_df = get_ts_df(ts_wide_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AutoGluon generates forecasts for each time series individually, without modeling interactions between different items/time series. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the dataset to train and test\n",
    "Split the dataset:\n",
    "- In the `train_df`, the last `prediction_length` time steps are removed from the end of each time series - meaning each time series contains the slice `[:-prediction_length]`\n",
    "- The `test_df` contains the same data as the original dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = ts_df.train_test_split(prediction_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize time series\n",
    "Visualize the `train_df` and `test_df` with the following interactive chart. You can play with these parameters:\n",
    "\n",
    "- `Time series id:`: id of the time series to plot\n",
    "- `Offset`: position of shown portion of the time series on the time axis. If `offset` is set to -1, the plot shows the tail of the dataset. If set to any other negative number, the plot shows the dataset starting from (`history length` + `offset`)*`prediction length` time steps from the end of the dataset. In other words, `offset` specifies how many `prediction_length` from the end of the dataset the time series is plotted\n",
    "- `History length`: how many `prediction lengths` of the time series are shown on the plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "style = {\"description_width\": \"initial\"}\n",
    "ts_id_list = test_df.item_ids.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@interact_manual(\n",
    "    ts_ids=Select(options=ts_id_list, value=ts_id_list[0], rows=5, description='Time series id:'),\n",
    "    offset=IntSlider(min=-100, max=-1, value=-1, style=style, description='Offset:'),\n",
    "    history_length=IntSlider(min=1, max=10, value=3, style=style, description='History length:'),\n",
    "    continuous_update=False,\n",
    ")\n",
    "def plot_interact(ts_ids, offset, history_length):\n",
    "    item_id = ts_ids\n",
    "    n_history = history_length\n",
    "    offset = -offset # prediction offset counted from the end of the time series in 'prediction_length' units.\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(nrows=2, figsize=[10, 6], sharex=True)\n",
    "    train_ts = train_df.slice_by_timestep(\n",
    "        start_index=-(n_history+offset)*prediction_length,\n",
    "        end_index=-prediction_length*offset if offset > 0 else None).loc[item_id]\n",
    "    test_ts = test_df.slice_by_timestep(\n",
    "        start_index=-(n_history+offset+1)*prediction_length,\n",
    "        end_index=-prediction_length*offset if offset > 0 else None).loc[item_id]\n",
    "    \n",
    "    ax1.set_title(f\"Train data: past time series values: {len(train_ts)} time steps shown\")\n",
    "    ax1.plot(train_ts)\n",
    "    ax2.set_title(f\"Test data: past + future time series values: {len(test_ts)} time steps shown\")\n",
    "    ax2.plot(test_ts)\n",
    "    ax2.tick_params(axis='x', labelrotation=90)\n",
    "    for ax in (ax1, ax2):\n",
    "        ax.fill_between(np.array([train_ts.index[-1], test_ts.index[-1]]), test_ts.min(), test_ts.max(), color=\"C1\", alpha=0.3, label=\"Forecast horizon = prediction length\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit models\n",
    "\n",
    "Use the AutoGluon [`TimeSeriesPredictor`](https://auto.gluon.ai/stable/api/autogluon.timeseries.TimeSeriesPredictor.html) class to train models and do inference.\n",
    "\n",
    "`TimeSeriesPredictor` provides probabilistic (quantile) multi-step-ahead forecasts for **univariate time series**. The forecast includes both the mean - conditional expectation of future values given the past, as well as the quantiles of the forecast distribution, indicating the range of possible future outcomes – probobalistic forecast.\n",
    "\n",
    "`TimeSeriesPredictor` fits both _global_ deep learning models that are shared across all time series such as DeepAR, Transformer, as well as _local_ statistical models that are fit to each individual time series such as ARIMA, ETS.\n",
    "\n",
    "You can include covariates known as dynamic features, exogenous variables, additional regressors or related time series into training. Examples of such covariates include holidays, promotions or weather forecasts. See the AutoGluon documentation for more details and examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp = strftime(\"%Y%m%d-%H%M%S\", gmtime())\n",
    "experiment_name = f\"{experiment_prefix}-{freq}-{ts_wide_df.shape[1]}-{ts_wide_df.shape[0]}\"\n",
    "eval_metric = 'WQL'\n",
    "forecast_quantiles = [0.1, 0.5, 0.9]\n",
    "predictor_data_path = f\"{experiment_name}-{timestamp}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_dataset = test_df.index.get_level_values(1)[0]\n",
    "end_dataset = test_df.index.get_level_values(1)[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = TimeSeriesPredictor(\n",
    "    target=\"target\",\n",
    "    prediction_length=prediction_length,\n",
    "    eval_metric=eval_metric,\n",
    "    quantile_levels=forecast_quantiles,\n",
    "    path=predictor_data_path, \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now run [`fit()`](https://auto.gluon.ai/stable/api/autogluon.timeseries.TimeSeriesPredictor.fit.html#autogluon.timeseries.TimeSeriesPredictor.fit) method to train models. \n",
    "\n",
    "\n",
    "You can provide a time limit in seconds in the parameter `time_limit` to fit the training in your time budget. If not provided, `fit()` will run until all models have completed training.\n",
    "\n",
    "Use the `presets` parameter to control a trade-off between the training speed and quality of results. Depending on the parameter value, AutoGluon can train simple statistical models and fast tree-based models for short training time or use all available models plus additional statistical models for more accurate predictions but longer training time. Note that Chronos has own presents in AutoGluon.\n",
    "\n",
    "If you're not familiar with AutoGluon, use `presets` and avoid specifying most other `fit()` parameters.\n",
    "\n",
    "If not specified otherwise in the parameter `enable_ensemble`, AutoGluon will fit a weighted ensemble on top of trained models.\n",
    "\n",
    "#### How AutoGluon creates a weighted ensemble\n",
    "\n",
    "The final step in AutoGluon's `fit` process is creation of a weighted ensemble:\n",
    "\n",
    "1. **Model Selection**: AutoGluon selects a subset of the trained models to include in the weighted ensemble. By default, it considers all previously trained models except for other weighted ensembles\n",
    "2. **Weight Optimization**: The algorithm then optimizes the weights assigned to each selected model. This is typically done using a simple linear model or another machine learning algorithm that can determine the optimal combination of model predictions\n",
    "3. **Performance Evaluation**: The weighted ensemble is evaluated on a held-out validation set to ensure it generalizes well to unseen data.\n",
    "\n",
    "Refer to the AutoGluon [documentation](https://auto.gluon.ai/dev/tutorials/tabular/how-it-works.html) for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "The training of the full dataset takes about 60 min on instances with 8 vCPU and and about 45 min on an instances with 16 vCPU. You can use <code>fast_training</code> preset for the lowest training time. The training of the small dataset with two time series takes about 15 min on <code>ml.m5.4xlarge</code>.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fit time by instance type\n",
    "This section provides some guidance on training times on various CPU and GPU instances. All training runs used `high_quality` presets. The full dataset contains 370 timeseries and 6.5 million rows, the small contains 2 time series and 35'000 rows. \n",
    "\n",
    "Tests used the following SageMaker [instance types](https://aws.amazon.com/sagemaker/pricing/):\n",
    "- CPU-instances: standard `ml.m7i.2xlarge`, `ml.m7i.4xlarge`, and `ml.m7i.24xlarge`, compute optimized `ml.c5.4xlarge` and `ml.c7i.2xlarge`, memory optimized `ml.r7i.2xlarge`\n",
    "- GPU-instances: Single GPU `ml.g5.2xlarge` and `ml.g5.4xlarge`, 4x GPU `ml.g5.12xlarge`\n",
    "\n",
    "The main factor for training time is the amount of CPUs. Both CPU and GPU instances with 16 vCPUs shown similar performance on the full dataset. For the small dataset, the GPU instances with 16 vCPU were about 60% faster than CPU instances. The recommended instance type for this dataset, both small and full versions, is `ml.g5.4xlarge`.\n",
    "\n",
    "<div class=\"alert alert-info\">Note that you need a GPU instance to use larger Chronos models.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now visualize the data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cpu_full = {\n",
    "    'instance_type': ['ml.c5.4xlarge', 'ml.c7i.2xlarge', 'ml.m7i.2xlarge', 'ml.m7i.4xlarge', 'ml.m7i.24xlarge', 'ml.r7i.2xlarge'],\n",
    "    'fit_time': [2671.17, 3969.44, 3863.30, 2241.30, 2386.65, 3864.86 ],\n",
    "}\n",
    "\n",
    "data_gpu_full = {\n",
    "    'instance_type': ['ml.g5.2xlarge', 'ml.g5.4xlarge', 'ml.g5.8xlarge', 'ml.g5.12xlarge'],\n",
    "    'fit_time': [3690.43, 2563.35, 2178.44, 1917.52],\n",
    "}\n",
    "\n",
    "data_cpu_small = {\n",
    "    'instance_type': ['ml.c5.4xlarge', 'ml.c7i.2xlarge', 'ml.m7i.2xlarge', 'ml.m7i.4xlarge', 'ml.m7i.24xlarge', 'ml.r7i.2xlarge'],\n",
    "    'fit_time': [815.22, 1098.92, 519.09, 443.99, 485.83, 751.10],  \n",
    "}\n",
    "\n",
    "data_gpu_small = {\n",
    "    'instance_type': ['ml.g5.2xlarge', 'ml.g5.4xlarge', 'ml.g5.12xlarge'],\n",
    "    'fit_time': [396.90, 328.97, 331.37], \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use Pricing API to get information about SageMaker instances\n",
    "pricing = boto3.client('pricing')\n",
    "service_code = 'AmazonSageMaker'\n",
    "attribute_name = 'instanceName'\n",
    "attr_list = ['computeType', 'vCpu', 'memory', 'gpu', 'physicalCpu',]\n",
    "\n",
    "# get instance attributes\n",
    "instance_data = {\n",
    "    i:{\n",
    "        a:json.loads(pricing.get_products(\n",
    "            ServiceCode=service_code,\n",
    "            Filters=json.loads(f'[{{\"Type\":\"TERM_MATCH\",\"Field\":\"{attribute_name}\",\"Value\":\"{i}\"}}]'),\n",
    "        )['PriceList'][0])['product']['attributes'][a] for a in attr_list\n",
    "    }\n",
    "    for i in data_cpu_full['instance_type'] + data_gpu_full['instance_type']\n",
    "}\n",
    "\n",
    "# Create the DataFrame\n",
    "instance_data_df = pd.DataFrame.from_dict(instance_data, orient='index').reset_index().rename(columns={'index': 'instance_type'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bar_chart(ax, data_cpu, data_gpu, title):\n",
    "    all_instance_types = data_cpu['instance_type'] + data_gpu['instance_type']\n",
    "    all_fit_times = data_cpu['fit_time'] + data_gpu['fit_time']\n",
    "\n",
    "    bar_width = 0.6\n",
    "    r1 = np.arange(len(all_instance_types))\n",
    "\n",
    "    bars = ax.bar(r1, all_fit_times, color=plt.cm.Set3(np.linspace(0, 1, len(all_instance_types))), width=bar_width)\n",
    "\n",
    "    ax.set_xlabel('Instance type')\n",
    "    ax.set_ylabel('Fit time (seconds)')\n",
    "    ax.set_title(title)\n",
    "    ax.set_xticks(r1)\n",
    "    ax.set_xticklabels(all_instance_types, rotation=45, ha='right')\n",
    "\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{height:.0f}',\n",
    "                ha='center', va='bottom', rotation=0)\n",
    "\n",
    "    cpu_patch = plt.Rectangle((0,0),1,1,fc=\"lightblue\", edgecolor = 'none')\n",
    "    gpu_patch = plt.Rectangle((0,0),1,1,fc=\"lightgreen\", edgecolor = 'none')\n",
    "    ax.legend([cpu_patch, gpu_patch], ['CPU instances', 'GPU instances'], loc='upper right')\n",
    "\n",
    "    for i, bar in enumerate(bars):\n",
    "        if i < len(data_cpu['instance_type']):\n",
    "            bar.set_facecolor('lightblue')\n",
    "        else:\n",
    "            bar.set_facecolor('lightgreen')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20, 16))\n",
    "gs = gridspec.GridSpec(2, 2, height_ratios=[1, 1])\n",
    "\n",
    "# Create the subplots\n",
    "ax1, ax2, ax3 = fig.add_subplot(gs[0, 0]), fig.add_subplot(gs[0, 1]), fig.add_subplot(gs[1, :])\n",
    "\n",
    "create_bar_chart(ax1, data_cpu_full, data_gpu_full, f'Fit time by instance type: full dataset {data_kw.shape[1]} time series')\n",
    "create_bar_chart(ax2, data_cpu_small, data_gpu_small, f'Fit time by instance type: small dataset {data_kw_small.shape[1]} time series')\n",
    "\n",
    "ax3.axis('off')\n",
    "table = ax3.table(cellText=instance_data_df.values, colLabels=instance_data_df.columns, loc='center', cellLoc='center')\n",
    "\n",
    "# Adjust table style\n",
    "table.auto_set_font_size(False)\n",
    "table.set_fontsize(12)\n",
    "table.scale(1, 1.5)\n",
    "\n",
    "# Adjust layout to prevent clipping of tick-labels\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fit\n",
    "Run the model training by calling `TimeSeriesPredictor.fit()`:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"data:image/svg+xml;base64,Cjxzdmcgd2lkdGg9IjgwMCIgaGVpZ2h0PSIxMjUiIHZpZXdCb3g9IjAgMCA4MDAgMTI1IiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciPgogICAgPGRlZnM+CiAgICAgICAgPGxpbmVhckdyYWRpZW50IGlkPSJmYWRlR3JhZGllbnQiIHgxPSIwIiB4Mj0iMSI+CiAgICAgICAgICAgIDxzdG9wIG9mZnNldD0iMCUiIHN0b3AtY29sb3I9IiNGMEYwRjAiLz4KICAgICAgICAgICAgPHN0b3Agb2Zmc2V0PSIxMDAlIiBzdG9wLWNvbG9yPSIjRjBGMEYwIiBzdG9wLW9wYWNpdHk9IjAiLz4KICAgICAgICA8L2xpbmVhckdyYWRpZW50PgogICAgICAgIDxtYXNrIGlkPSJmYWRlTWFzayI+CiAgICAgICAgICAgIDxyZWN0IHg9IjAiIHk9IjAiIHdpZHRoPSI3NTAiIGhlaWdodD0iMTI1IiBmaWxsPSJ3aGl0ZSIvPgogICAgICAgICAgICA8cmVjdCB4PSI3NTAiIHk9IjAiIHdpZHRoPSI1MCIgaGVpZ2h0PSIxMjUiIGZpbGw9InVybCgjZmFkZUdyYWRpZW50KSIvPgogICAgICAgIDwvbWFzaz4KICAgIDwvZGVmcz4KICAgIDxwYXRoIGQ9Ik0zLDUwIEE1MCw1MCAwIDAgMSA1MywzIEw3OTcsMyBMNzk3LDk3IEw5Nyw5NyBMNTAsMTE1IEwzLDk3IFoiIGZpbGw9IiNGMEYwRjAiIHN0cm9rZT0iI0UwRTBFMCIgc3Ryb2tlLXdpZHRoPSIxIiBtYXNrPSJ1cmwoI2ZhZGVNYXNrKSIvPgogICAgPGNpcmNsZSBjeD0iNTAiIGN5PSI1MCIgcj0iMzAiIGZpbGw9IiM1N2M0ZjgiIHN0cm9rZT0iIzU3YzRmOCIgc3Ryb2tlLXdpZHRoPSIxIi8+CiAgICA8Y2lyY2xlIGN4PSI1MCIgY3k9IjUwIiByPSIyNSIgZmlsbD0iI0YwRjBGMCIvPgogICAgPGxpbmUgeDE9IjUwIiB5MT0iNTAiIHgyPSI1MCIgeTI9IjMwIiBzdHJva2U9IiM1N2M0ZjgiIHN0cm9rZS13aWR0aD0iMyIgc3Ryb2tlLWxpbmVjYXA9InJvdW5kIi8+CiAgICA8bGluZSB4MT0iNTAiIHkxPSI1MCIgeDI9IjY1IiB5Mj0iNTAiIHN0cm9rZT0iIzU3YzRmOCIgc3Ryb2tlLXdpZHRoPSIzIiBzdHJva2UtbGluZWNhcD0icm91bmQiLz4KICAgIDx0ZXh0IHg9IjEwMCIgeT0iMzQiIGZvbnQtZmFtaWx5PSJBcmlhbCwgc2Fucy1zZXJpZiIgZm9udC1zaXplPSIxNCIgZmlsbD0iIzMzMzMzMyI+VGhlIG5leHQgY2VsbCBtYXkgdGFrZSBhIGZldyBtaW51dGVzIHRvIHJ1bi48L3RleHQ+Cjwvc3ZnPgo=\" alt=\"Time alert open medium\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace presets with \"fast_training\" for the lowest training time\n",
    "presets = \"high_quality\" # \"fast_training\"\n",
    "\n",
    "predictor.fit(\n",
    "    train_data=train_df,\n",
    "    # time_limit=30*60,\n",
    "    presets=presets,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"data:image/svg+xml;base64,Cjxzdmcgd2lkdGg9IjgwMCIgaGVpZ2h0PSI1MCIgdmlld0JveD0iMCAwIDgwMCA1MCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICAgIDxkZWZzPgogICAgICAgIDxsaW5lYXJHcmFkaWVudCBpZD0iZmFkZUdyYWRpZW50IiB4MT0iMCIgeDI9IjEiPgogICAgICAgICAgICA8c3RvcCBvZmZzZXQ9IjAlIiBzdG9wLWNvbG9yPSIjRjBGMEYwIi8+CiAgICAgICAgICAgIDxzdG9wIG9mZnNldD0iMTAwJSIgc3RvcC1jb2xvcj0iI0YwRjBGMCIgc3RvcC1vcGFjaXR5PSIwIi8+CiAgICAgICAgPC9saW5lYXJHcmFkaWVudD4KICAgICAgICA8bWFzayBpZD0iZmFkZU1hc2siPgogICAgICAgICAgICA8cmVjdCB4PSIwIiB5PSIwIiB3aWR0aD0iNzUwIiBoZWlnaHQ9IjUwIiBmaWxsPSJ3aGl0ZSIvPgogICAgICAgICAgICA8cmVjdCB4PSI3NTAiIHk9IjAiIHdpZHRoPSI1MCIgaGVpZ2h0PSI1MCIgZmlsbD0idXJsKCNmYWRlR3JhZGllbnQpIi8+CiAgICAgICAgPC9tYXNrPgogICAgPC9kZWZzPgogICAgPHBhdGggZD0iTTI1LDUwIFEwLDUwIDAsMjUgTDUwLDMgTDk3LDI1IEw3OTcsMjUgTDc5Nyw1MCBMMjUsNTAgWiIgZmlsbD0iI0YwRjBGMCIgc3Ryb2tlPSIjRTBFMEUwIiBzdHJva2Utd2lkdGg9IjEiIG1hc2s9InVybCgjZmFkZU1hc2spIi8+Cjwvc3ZnPgo=\" alt=\"Time alert close\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with trained predictor\n",
    "\n",
    "After training completed, you can access all information via `TimeSeriesPredictor` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed information about models trained\n",
    "predictor.fit_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary of model details\n",
    "predictor.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See the AutoGluon documentation on the [`leaderboard()`](https://auto.gluon.ai/stable/api/autogluon.timeseries.TimeSeriesPredictor.leaderboard.html) method for description of each shown column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model leaderboard with scoring\n",
    "predictor.leaderboard()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <img src=\"data:image/svg+xml;base64,Cjxzdmcgd2lkdGg9IjgwMCIgaGVpZ2h0PSIxMjUiIHZpZXdCb3g9IjAgMCA4MDAgMTI1IiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciPgogICAgPGRlZnM+CiAgICAgICAgPGxpbmVhckdyYWRpZW50IGlkPSJmYWRlR3JhZGllbnQiIHgxPSIwIiB4Mj0iMSI+CiAgICAgICAgICAgIDxzdG9wIG9mZnNldD0iMCUiIHN0b3AtY29sb3I9IiNGMEYwRjAiLz4KICAgICAgICAgICAgPHN0b3Agb2Zmc2V0PSIxMDAlIiBzdG9wLWNvbG9yPSIjRjBGMEYwIiBzdG9wLW9wYWNpdHk9IjAiLz4KICAgICAgICA8L2xpbmVhckdyYWRpZW50PgogICAgICAgIDxtYXNrIGlkPSJmYWRlTWFzayI+CiAgICAgICAgICAgIDxyZWN0IHg9IjAiIHk9IjAiIHdpZHRoPSI3NTAiIGhlaWdodD0iMTI1IiBmaWxsPSJ3aGl0ZSIvPgogICAgICAgICAgICA8cmVjdCB4PSI3NTAiIHk9IjAiIHdpZHRoPSI1MCIgaGVpZ2h0PSIxMjUiIGZpbGw9InVybCgjZmFkZUdyYWRpZW50KSIvPgogICAgICAgIDwvbWFzaz4KICAgIDwvZGVmcz4KICAgIDxwYXRoIGQ9Ik0zLDUwIEE1MCw1MCAwIDAgMSA1MywzIEw3OTcsMyBMNzk3LDk3IEw5Nyw5NyBMNTAsMTE1IEwzLDk3IFoiIGZpbGw9IiNGMEYwRjAiIHN0cm9rZT0iI0UwRTBFMCIgc3Ryb2tlLXdpZHRoPSIxIiBtYXNrPSJ1cmwoI2ZhZGVNYXNrKSIvPgogICAgPGNpcmNsZSBjeD0iNTAiIGN5PSI1MCIgcj0iMzAiIGZpbGw9IiM1N2M0ZjgiIHN0cm9rZT0iIzU3YzRmOCIgc3Ryb2tlLXdpZHRoPSIxIi8+CiAgICA8Y2lyY2xlIGN4PSI1MCIgY3k9IjUwIiByPSIyNSIgZmlsbD0iI0YwRjBGMCIvPgogICAgPGxpbmUgeDE9IjUwIiB5MT0iNTAiIHgyPSI1MCIgeTI9IjMwIiBzdHJva2U9IiM1N2M0ZjgiIHN0cm9rZS13aWR0aD0iMyIgc3Ryb2tlLWxpbmVjYXA9InJvdW5kIi8+CiAgICA8bGluZSB4MT0iNTAiIHkxPSI1MCIgeDI9IjY1IiB5Mj0iNTAiIHN0cm9rZT0iIzU3YzRmOCIgc3Ryb2tlLXdpZHRoPSIzIiBzdHJva2UtbGluZWNhcD0icm91bmQiLz4KICAgIDx0ZXh0IHg9IjEwMCIgeT0iMzQiIGZvbnQtZmFtaWx5PSJBcmlhbCwgc2Fucy1zZXJpZiIgZm9udC1zaXplPSIxNCIgZmlsbD0iIzMzMzMzMyI+VGhlIG5leHQgY2VsbCBtYXkgdGFrZSBhIGZldyBtaW51dGVzIHRvIHJ1bi48L3RleHQ+Cjwvc3ZnPgo=\" alt=\"Time alert open medium\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can provide a dataset to evaluate the model leaderboard based on the test data\n",
    "predictor.leaderboard(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"data:image/svg+xml;base64,Cjxzdmcgd2lkdGg9IjgwMCIgaGVpZ2h0PSI1MCIgdmlld0JveD0iMCAwIDgwMCA1MCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICAgIDxkZWZzPgogICAgICAgIDxsaW5lYXJHcmFkaWVudCBpZD0iZmFkZUdyYWRpZW50IiB4MT0iMCIgeDI9IjEiPgogICAgICAgICAgICA8c3RvcCBvZmZzZXQ9IjAlIiBzdG9wLWNvbG9yPSIjRjBGMEYwIi8+CiAgICAgICAgICAgIDxzdG9wIG9mZnNldD0iMTAwJSIgc3RvcC1jb2xvcj0iI0YwRjBGMCIgc3RvcC1vcGFjaXR5PSIwIi8+CiAgICAgICAgPC9saW5lYXJHcmFkaWVudD4KICAgICAgICA8bWFzayBpZD0iZmFkZU1hc2siPgogICAgICAgICAgICA8cmVjdCB4PSIwIiB5PSIwIiB3aWR0aD0iNzUwIiBoZWlnaHQ9IjUwIiBmaWxsPSJ3aGl0ZSIvPgogICAgICAgICAgICA8cmVjdCB4PSI3NTAiIHk9IjAiIHdpZHRoPSI1MCIgaGVpZ2h0PSI1MCIgZmlsbD0idXJsKCNmYWRlR3JhZGllbnQpIi8+CiAgICAgICAgPC9tYXNrPgogICAgPC9kZWZzPgogICAgPHBhdGggZD0iTTI1LDUwIFEwLDUwIDAsMjUgTDUwLDMgTDk3LDI1IEw3OTcsMjUgTDc5Nyw1MCBMMjUsNTAgWiIgZmlsbD0iI0YwRjBGMCIgc3Ryb2tlPSIjRTBFMEUwIiBzdHJva2Utd2lkdGg9IjEiIG1hc2s9InVybCgjZmFkZU1hc2spIi8+Cjwvc3ZnPgo=\" alt=\"Time alert close\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To load all saved predictor data including the model leaderboard, logs, and the trained model you use the [`load()`](https://auto.gluon.ai/stable/api/autogluon.timeseries.TimeSeriesPredictor.load.html) method of the `TimeSeriesPredictor` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list all autogluon data saved to the EBS volume\n",
    "!ls -dt {experiment_prefix}*/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment if you need to load a predictor from the saved data\n",
    "# predictor_data_path=\"autogluon-2H-370-17533-20240905-214138\"\n",
    "# predictor = TimeSeriesPredictor.load(predictor_data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model performance\n",
    "\n",
    "To measure how accurately `TimeSeriesPredictor` can forecast unseen time series, you can use the method [`TimeSeriesPredictor.evaluate()`](https://auto.gluon.ai/stable/api/autogluon.timeseries.TimeSeriesPredictor.evaluate.html). This method measures the model accuracy using the last `prediction_length` time steps of each time series in data as a hold-out set, in this case in the dataset `test_df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = ['WQL', 'MAPE', 'WAPE', 'RMSE', 'MASE']\n",
    "# You can also specify a model name via `model` parameter\n",
    "model_metrics = predictor.evaluate(\n",
    "    data=test_df, \n",
    "    metrics=metrics, \n",
    "    # model=\"DeepAR\", \n",
    "    display=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save the model performance to a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_and_save_metrics(\n",
    "    model_metrics,\n",
    "    experiment_name,\n",
    "    timestamp=strftime(\"%Y%m%d-%H%M%S\", gmtime()),\n",
    "    display_only=False,\n",
    "):\n",
    "    model_metrics_df = pd.DataFrame.from_dict(model_metrics, orient='index', columns=['value']).reset_index().rename(columns={'index': 'metric_name'})\n",
    "    model_metrics_df['experiment'] = experiment_name\n",
    "    model_metrics_df['timestamp'] = timestamp\n",
    "    # AutoGluon flips the sign of a metric, multiply by -1\n",
    "    model_metrics_df['value'] = model_metrics_df['value'] * (-1)\n",
    "    model_metrics_df = model_metrics_df[['timestamp','metric_name','value','experiment']]\n",
    "\n",
    "    print(model_metrics_df)\n",
    "\n",
    "    if not display_only:\n",
    "        model_metrics_df.to_csv(f'./model-performance/{experiment_name}-{timestamp}.csv', index=False)\n",
    "\n",
    "    return model_metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"./model-performance\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_and_save_metrics(model_metrics, experiment_name, timestamp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate predictions and visualize\n",
    "\n",
    "To generate predictions and to plot the results you use the methonds [`predict()`](https://auto.gluon.ai/stable/api/autogluon.timeseries.TimeSeriesPredictor.predict.html) and [`plot()`](https://auto.gluon.ai/stable/api/autogluon.timeseries.TimeSeriesPredictor.plot.html) of the `TimeSeriesPredictor`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# persist models in memory for reduced inference latency\n",
    "predictor.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_df = predictor.predict(test_df.slice_by_timestep(end_index=-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`predict()` method generates `prediction_length` starting from the end of the provided time series:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`plot()` method plots predictions together with historical time series and confidence interval based on forecast quantiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.plot(\n",
    "    data=test_df.slice_by_timestep(end_index=-1),\n",
    "    predictions=prediction_df,\n",
    "    quantile_levels=forecast_quantiles,\n",
    "    item_ids=prediction_df.item_ids[0:2],\n",
    "    max_history_length=prediction_length*3,   \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment with different trained models and prediction dates to generate forecast. You can change the following parameters:\n",
    "\n",
    "- `Model`: select any model from the predictor's leaderboard\n",
    "- `Time series ids`: id of the time series to forecast. You can select multiple time series to plot together\n",
    "- `Offset days`: how many days back from the data end the prediction starts\n",
    "- `History lenght`: how many 'prediction lengths' of history are shown on the plot as the history series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "style = {\"description_width\": \"initial\"}\n",
    "ts_id_list = test_df.item_ids.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@interact_manual(\n",
    "    model=Dropdown(options=predictor.leaderboard()['model'].to_list(), description='Model:'),\n",
    "    ts_ids=SelectMultiple(options=ts_id_list,value=[ts_id_list[0]], rows=5, style=style, description='Time series ids:'),\n",
    "    offset_days=IntSlider(min=-365, max=0, value=0, style=style, description='Offset days:'),\n",
    "    history_length=IntSlider(min=1, max=10, value=3, style=style, description='History length:'),\n",
    "    continuous_update=False,\n",
    ")\n",
    "def plot_interact(\n",
    "    model,\n",
    "    ts_ids,\n",
    "    offset_days,\n",
    "    history_length,\n",
    "):\n",
    "    offset = -offset_days * intervals_per_day + 1\n",
    "    data_end_index = offset - prediction_length + 1\n",
    "\n",
    "    data_df = test_df.slice_by_timestep(end_index=-data_end_index) if offset > prediction_length else test_df\n",
    "    inference_df = test_df.slice_by_timestep(end_index=-offset)\n",
    "    prediction_df = predictor.predict(data=inference_df, model=model)\n",
    "        \n",
    "    predictor.plot(\n",
    "        data=data_df,\n",
    "        predictions=prediction_df,\n",
    "        quantile_levels=forecast_quantiles,\n",
    "        item_ids=list(ts_ids),\n",
    "        max_history_length=history_length*prediction_length,   \n",
    "    )\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4xXRcOPEcTAh"
   },
   "source": [
    "## Use Chronos with AutoGluon\n",
    "\n",
    "This section uses content from the AutoGluon documentation [Forecasting with Chronos](https://auto.gluon.ai/stable/tutorials/timeseries/forecasting-chronos.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">If you run this notebook on a CPU-only instance, you can use Chronos models <code>tiny</code> and <code>mini</code> only.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chronos on AutoGluon is available in five model sizes with different [numbers of parameters](https://github.com/amazon-science/chronos-forecasting/tree/main?tab=readme-ov-file#architecture): `tiny` (8M), `mini` (20M), `small` (46M), `base` (200M), and `large` (710M). \n",
    "\n",
    "Being a pretrained transformer model for zero-shot forecasting, Chronos is different from other models available in AutoGluon Time Series. Chronos models do not really fit time series data during training time with `fit()`. When `predict()` is called, it carries out a relatively more expensive computation that scales linearly with the number of time series in the dataset. In this aspect, they behave like local statistical models such as ETS or ARIMA, or generative AI foundation models where expensive computation happens during inference. Computation for `small`, `base`, and `large` Chronos models requires a GPU accelerator chip to run in a reasonable amount of time.\n",
    "\n",
    "The easiest way to get started with Chronos is through model-specific presets available in the [`TimeSeriesPredictor.fit()`](https://auto.gluon.ai/stable/api/autogluon.timeseries.TimeSeriesPredictor.fit.html). As of v1.1, the `fit()` method has five specific Chronos presets corresponding to each model size, such as `chronos_small` or `chronos_base` and two Chronos-specific ensemble presets.\n",
    "\n",
    "In ensemble mode Chronos `base` or `large` models are combined with other time series models using presets `chronos_ensemble` and `chronos_large_ensemble`. More details about these presets are available in the documentation for [`TimeSeriesPredictor.fit()`](https://auto.gluon.ai/stable/api/autogluon.timeseries.TimeSeriesPredictor.fit.html).\n",
    "\n",
    "Note that the model sizes `small` and higher require a GPU to run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1AOUWSvccTAi"
   },
   "source": [
    "### Create a predictor\n",
    "Create a `TimeSeriesPredictor` for Chronos usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sODP__wncTAi",
    "outputId": "fcc3c250-4206-47fd-b567-3e24630c0de5"
   },
   "outputs": [],
   "source": [
    "train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chronos_predictor = TimeSeriesPredictor(\n",
    "    target=\"target\",\n",
    "    prediction_length=prediction_length,\n",
    "    eval_metric=eval_metric,\n",
    "    quantile_levels=forecast_quantiles,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit Chronos models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_gpus = torch.cuda.device_count()\n",
    "print(f\"Number of GPUs available: {number_of_gpus}\")\n",
    "\n",
    "if number_of_gpus < 1:\n",
    "    chronos_size = 'mini'\n",
    "elif number_of_gpus == 1:\n",
    "    chronos_size = 'base'\n",
    "else:\n",
    "    chronos_size = 'large'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chronos_predictor.fit(\n",
    "    train_data=train_df,\n",
    "    presets=f\"chronos_{chronos_size}\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chronos_predictor.leaderboard()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YVzKNV_rcTAk"
   },
   "source": [
    "As you can see, Chronos does not take any time to `fit`. The `fit` still does some of tasks under the hood, such as inferring the frequency of time series and saving the predictor's state to disk."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate predictions and visualize\n",
    "\n",
    "Use the `predict()` method to generate forecasts, and the `plot()` method to visualize them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZudLS8CFcTAk",
    "outputId": "3409c24a-c06b-48b2-f44c-37b0eb56b31a"
   },
   "outputs": [],
   "source": [
    "model = chronos_predictor.leaderboard()['model'][0]\n",
    "\n",
    "# remove the last time step in the test dataset because of the artifact where the target goes to zero\n",
    "chronos_prediction_df = chronos_predictor.predict(data=test_df.slice_by_timestep(end_index=-1), model=model)\n",
    "\n",
    "chronos_predictor.plot(\n",
    "    data=test_df,\n",
    "    predictions=chronos_prediction_df,\n",
    "    quantile_levels=forecast_quantiles,\n",
    "    item_ids=chronos_prediction_df.item_ids[:2],\n",
    "    max_history_length=prediction_length*3,   \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chronos model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = ['WQL', 'MAPE', 'WAPE', 'RMSE', 'MASE']\n",
    "chronos_model_metrics = chronos_predictor.evaluate(\n",
    "    data=test_df.slice_by_timestep(end_index=-1), \n",
    "    metrics=metrics, \n",
    "    model=model,\n",
    "    display=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp = strftime(\"%Y%m%d-%H%M%S\", gmtime())\n",
    "chronos_experiment_name = f\"{experiment_prefix}-{model}-{freq}-{ts_wide_df.shape[1]}-{ts_wide_df.shape[0]}\"\n",
    "chronos_predictor_data_path = f\"{chronos_experiment_name}-{timestamp}\"\n",
    "\n",
    "print(f\"Experiment name: {chronos_experiment_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_and_save_metrics(chronos_model_metrics, chronos_experiment_name, timestamp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Backtesting using multiple windows\n",
    "You can more accurately estimate the performance using backtest with multiple windows by evaluating performance on multiple forecast horizons generated from the same time series.\n",
    "The AutoGluon class `ExpandingWindowSplitter` provides the needed functionality to split the dataset.\n",
    "\n",
    "Multi-window backtesting can result in more accurate estimation of the forecast quality on **unseen** data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_windows = 4\n",
    "splitter = ExpandingWindowSplitter(prediction_length=prediction_length, num_val_windows=n_windows)\n",
    "item_ids = test_df.item_ids[:2]\n",
    "scores = []\n",
    "\n",
    "for window_idx, (train_split, val_split) in enumerate(splitter.split(test_df.slice_by_timestep(end_index=-1))):\n",
    "    # predict\n",
    "    split_prediction_df = chronos_predictor.predict(data=val_split.slice_by_timestep(end_index=-prediction_length), model=model)\n",
    "    # visualize\n",
    "    chronos_predictor.plot(\n",
    "        data=test_df.slice_by_timestep(start_index=-n_windows*prediction_length),\n",
    "        predictions=split_prediction_df,\n",
    "        item_ids=item_ids,\n",
    "    )\n",
    "    # evaluate\n",
    "    score = chronos_predictor.evaluate(\n",
    "        data=val_split,\n",
    "        metrics=metrics,\n",
    "        model=model,\n",
    "    )\n",
    "    \n",
    "    print(f\"Window {window_idx}: score = {score}\")\n",
    "    scores.append(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save the model performance to a file\n",
    "Save the scoring from the backtest with multiple windows to a file. The simplest method to calculate an aggregated metric for backtest with multiple windows is to take the arithmetic mean across all windows. This gives equal weight to each window. You can also use the median metric value across windows that can be more robust to outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_and_save_metrics(\n",
    "    model_metrics=pd.DataFrame(data=scores).mean().to_dict(), \n",
    "    experiment_name=f\"{chronos_experiment_name}-bt{n_windows}\", \n",
    "    timestamp=strftime(\"%Y%m%d-%H%M%S\", gmtime())\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zJpEPgg_cTAl"
   },
   "source": [
    "### Optional: configuring for performance\n",
    "\n",
    "As with all large deep learning models some fine-grained control of inference parameters can be needed to both optimize the speed and avoid out-of-memory issues on specific hardware. For this, we will need to dive a bit deeper, configuring `hyperparameters` of the `TimeSeriesPredictor` directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z_mQkdSHcTAl"
   },
   "outputs": [],
   "source": [
    "chronos_cpu_predictor = TimeSeriesPredictor(prediction_length=prediction_length).fit(\n",
    "    train_df,\n",
    "    hyperparameters={\n",
    "        \"Chronos\": {\n",
    "            \"model_path\": \"mini\",\n",
    "            \"batch_size\": 64,\n",
    "            \"device\": \"cpu\"\n",
    "        }\n",
    "    },\n",
    "    skip_model_selection=True,\n",
    "    verbosity=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2tyddAI8cTAl",
    "outputId": "f4814741-bef1-4d90-f5eb-c4ec088806c0"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "predictions = chronos_cpu_predictor.predict(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-tMfoQchcTAm"
   },
   "source": [
    "You used the following configuration options for the `TimeSeriesPredictor`:\n",
    "- `skip_model_selection=True` to skip running backtests during `fit`, as you use a single model only.\n",
    "- in the `hyperparameters` for the Chronos model,\n",
    "    - `model_path` allows to change the model size or select different pretrained weights. This parameter can be a model string like `tiny` or `base`, a Hugging Face path like `amazon/chronos-t5-mini`, or a path to a local folder with custom weights.\n",
    "    - `batch_size` configures the number of time series for which predictions are generated in parallel.\n",
    "    - `device` instructs Chronos to run the model on CPU.\n",
    "\n",
    "If you run this notebook on a GPU-instances, you see that inference speed is slower on the CPU. To improve inference speed AutoGluon implementation of Chronos supports several deep learning compilers that can optimize model performance on CPUs.\n",
    "\n",
    "For example, you can set `optimization_strategy=\"openvino\"` to use the [OpenVINO](https://github.com/openvinotoolkit/openvino) compiler for Intel CPUs to speed up Chronos inference. Behind the scenes, AutoGluon will use Hugging Face [optimum](https://github.com/huggingface/optimum-intel) for this conversion.\n",
    "\n",
    "Note that this requires installing the optional OpenVINO dependency for AutoGluon Time Series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cBrWz6YMcTAm"
   },
   "outputs": [],
   "source": [
    "!pip install -q \"autogluon.timeseries[chronos-openvino]\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JJZv3hLmcTAm"
   },
   "source": [
    "To speed up the inference even further, we can `persist` the model after calling `fit`. The `TimeSeriesPredictor.persist()` method tells AutoGluon to keep the Chronos model in device memory for fast, on-demand inference instead of loading the model from disk each time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dmDhl5DkcTAm"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "fast_chronos_cpu_predictor = TimeSeriesPredictor(prediction_length=prediction_length).fit(\n",
    "    train_df,\n",
    "    hyperparameters={\n",
    "        \"Chronos\": {\n",
    "            \"model_path\": \"tiny\",\n",
    "            \"batch_size\": 64,\n",
    "            \"device\": \"cpu\",\n",
    "            \"optimization_strategy\": \"openvino\",\n",
    "        }\n",
    "    },\n",
    "    skip_model_selection=True,\n",
    "    verbosity=0,\n",
    ")\n",
    "fast_chronos_cpu_predictor.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6EegtRchcTAm",
    "outputId": "8cc5aa28-1cec-44b7-cc84-657352b75c27"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "predictions = fast_chronos_cpu_predictor.predict(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O0JFLbMhcTAn"
   },
   "source": [
    "Depending on your environment and the used JupyterLab App instance you might see an increase in inference time.\n",
    "\n",
    "You can also use the ONNX runtime by providing `optimization_strategy=\"onnx\"`. For a discussion of these and other hyperparameters of Chronos, see the Chronos model [documentation](forecasting-model-zoo.md)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further Chronos experiments\n",
    "\n",
    "You can use another Chronos-specific presets as described in the [`TimeSeriesPredictor.fit()`](https://auto.gluon.ai/stable/api/autogluon.timeseries.TimeSeriesPredictor.fit.html) documentation:\n",
    "- `chronos_ensemble`: builds an ensemble of seasonal naive, tree-based and deep learning models with fast inference and `chronos_small`.    \n",
    "- `chronos_large_ensemble`: builds an ensemble of seasonal naive, tree-based and deep learning models with fast inference and `chronos_large`.\n",
    "\n",
    "Feel free to experiment with these presets using code from this notebook.\n",
    "\n",
    "<div class=\"alert alert-info\">Note you can use these presets to fit models and run inference on GPU instances only.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AutoGluon Cloud\n",
    "\n",
    "[AutoGluon Cloud](https://auto.gluon.ai/0.8.1/tutorials/cloud_fit_deploy/autogluon-cloud.html) is a framework designed to simplify the process of training, fine-tuning, and deploying AutoGluon-backed models on AWS. It aims to provide users with a straightforward way to manage MLOps without delving into the complexities of resource management. With AutoGluon Cloud, you can train models and perform inference on the cloud using just a few lines of code.\n",
    "\n",
    "Currently, AutoGluon Cloud supports SageMaker as its cloud backend, allowing users to leverage SageMaker's capabilities for training and deploying models. The `autogluon.cloud` module provides APIs to use SageMaker containers for training and deploying AutoGluon-backed models. This includes support for tabular, multimodal, text, and image predictors. You can install [`autogluon.cloud`]((https://pypi.org/project/autogluon.cloud/)) via `pip` and then use it to train and deploy models, with compute managed by SageMaker and storage handled by Amazon S3.\n",
    "\n",
    "For time series forecasting AutoGluon Cloud offers the [`TimeSeriesCloudPredictor`](https://auto.gluon.ai/cloud/dev/api/autogluon.cloud.TimeSeriesCloudPredictor.html). With `TimeSeriesCloudPredictor` you can train model remotely on SageMaker-managed compute using SageMaker training jobs.\n",
    "\n",
    "Using AutoGluon Cloud can further simplify and scale development of time series forecast models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nOcBQzVncTAn"
   },
   "source": [
    "## Clean up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# free memory\n",
    "predictor.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shutdown kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%html\n",
    "\n",
    "<p><b>Shutting down your kernel for this notebook to release resources.</b></p>\n",
    "<button class=\"sm-command-button\" data-commandlinker-command=\"kernelmenu:shutdown\" style=\"display:none;\">Shutdown Kernel</button>\n",
    "        \n",
    "<script>\n",
    "try {\n",
    "    els = document.getElementsByClassName(\"sm-command-button\");\n",
    "    els[0].click();\n",
    "}\n",
    "catch(err) {\n",
    "    // NoOp\n",
    "}    \n",
    "</script>"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
